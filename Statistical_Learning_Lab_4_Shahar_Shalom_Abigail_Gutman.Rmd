---
title: "Statistical learning lab 4 - Classification"
author: 'Abigail Gutman and Shahar Shalom '
date: "29/6/2021"
output: html_document
---

```{r message=FALSE, warning=FALSE, include=FALSE}
options(scipen = 999)

library(ggplot2)
library(dplyr)
library(tidyverse)
library(readr)
library(readxl)
library(glmnet)
library(grid)
library(gridExtra)
library(cowplot)
library(xgboost)
library(caret)
library(rsample) 
```


# **Classification Lab**

We will try to classify handwritten digits in 28x28 greyscale values by their digit. The images are from the
MNIST dataset; you can (and should) read more about the dataset here: http://yann.lecun.com/exdb/mnist/.

For your convenience, I have supplied a script (written by Prof. David Dalpiaz from UIUC) that downloads
the data and prepares it for R. load MNIST.R

Your goal is to build a classifier for telling apart the digit 8 from the digit 3. The data includes all digits,
so first separate the required digits from the training set and the test set. Please use only the 4000 images
from the training set for fitting.

## prep: 
```{r}
# modification of https://gist.github.com/brendano/39760
# automatically obtains data from the web
# creates two data frames, test and train
# labels are stored in the y variables of each data frame
# can easily train many models using formula `y ~ .` syntax

# download data from http://yann.lecun.com/exdb/mnist/
download.file("http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz",
              "train-images-idx3-ubyte.gz")
download.file("http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz",
              "train-labels-idx1-ubyte.gz")
download.file("http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz",
              "t10k-images-idx3-ubyte.gz")
download.file("http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz",
              "t10k-labels-idx1-ubyte.gz")

# gunzip the files
R.utils::gunzip("train-images-idx3-ubyte.gz")
R.utils::gunzip("train-labels-idx1-ubyte.gz")
R.utils::gunzip("t10k-images-idx3-ubyte.gz")
R.utils::gunzip("t10k-labels-idx1-ubyte.gz")

# helper function for visualization
show_digit = function(arr784, col = gray(12:1 / 12), ...) {
  image(matrix(as.matrix(arr784[-785]), nrow = 28)[, 28:1], col = col, ...)
}
```


```{r}

# load image files
load_image_file = function(filename) {
  ret = list()
  f = file(filename, 'rb')
  readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
  close(f)
  data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
}

# load label files
load_label_file = function(filename) {
  f = file(filename, 'rb')
  readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
  close(f)
  y
}

# load images
train = load_image_file("train-images-idx3-ubyte")
test  = load_image_file("t10k-images-idx3-ubyte")

# load labels
train$y = as.factor(load_label_file("train-labels-idx1-ubyte"))
test$y  = as.factor(load_label_file("t10k-labels-idx1-ubyte"))

# view test image
#show_digit(train[10000, ])
```

## Q1:

I’d like you to compare two methods, each from a different classifier family (e.g. Classification trees,
Bayesian classifiers, linear discriminants, etc). You can use as features the raw pixel values (0 for white
to 255 for black). 
Explain your decisions in specifying the penalty and any hyper-parameters, and
in choosing the threshold.

נרצה להשוות בין שתי שיטות שונות של קלסיפיקציה, 
```{r}
# use only 4000 for training. train on the same proportion of y
zero_var <- round(apply(train[,-785],2,var),5)
zero_var <- zero_var != 0
p <- c(which(zero_var == T),785)
test <- test[,p] %>% filter(y %in% list(3,8))
test[1:784,] <- test[1:784,]
train <- train[,p] %>% filter(y %in% list(3,8))
train[1:784,] <- train[1:784,]
train$y <- as.factor(train$y)
set.seed(1342)
train_sampling <- initial_split(train,prop=4000/length(train[,1]), strata="y")
train_sampling$id
train4000 <- training(train_sampling)
train4000$y <- factor(make.names(train4000$y))
test$y <- factor(make.names(test$y))

#train4000 %>% group_by(y) %>% count()

```

###Model 1 : Naive bayes classifier model
```{r}
# estimating G from the data #presntaion 11a # check kernel, and band # non-parmetric
naive_bayes_model <- train(y ~., data = train4000 ,method = 'naive_bayes',
                           trControl = trainControl(classProbs = TRUE,savePredictions= "final"))

preds_naive_bayes <- predict(naive_bayes_model, newdata = test)

```


###Model 2 : gardient boosting - classifiar
```{r}

gbm_model_hyperparmeters <- train(y ~ ., data = train4000, method = 'xgbTree',
                                  trControl = trainControl("cv", number = 3,
                                                           classProbs = TRUE,savePredictions= "final"))
gbm_model_hyperparmeters$bestTune
```

```{r}
pred_gbm_model <- predict(gbm_model_hyperparmeters,test)
```


## Q2:

Write your own function that calculates (a) the confusion matrix (b) the precision and (c) the recall
(assume the class 3 is positive). Calculate the values for both classifiers. Do you observe overfitting of
the classifier?

```{r}
confusion_matrix_fun <- function(Y,Yhat){
  Negative_line <- Yhat[which(Y == 'X8')]
  Postive_line <- Yhat[which(Y == 'X3')]
  Negative_line <- c(sum(Negative_line == 'X8'),
                     sum(Negative_line == 'X3'),
                     sum(Negative_line == 'X8')/length(Negative_line),
                     sum(Negative_line == 'X3')/length(Negative_line))
  Postive_line <- c(sum(Postive_line == 'X8'),
                     sum(Postive_line == 'X3'),
                     sum(Postive_line == 'X8')/length(Postive_line),
                     sum(Postive_line == 'X3')/length(Postive_line))
  
  
  confus <- rbind(Negative_line,Postive_line)
  colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
  row.names(confus) <- c("Negtive (8)","Postive (3)")
  Pr <- confus[2,2] / sum(confus[,2])
  Re <- confus[2,2] / sum(confus[2,])
  Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
  Se <- confus[2,2] / (confus[2,2] + confus[2,1]) # TP / TP+FN
  sp <- confus[1,1] / (confus[1,1] + confus[1,2]) # TN/ TN+FP
  
  return(list(confusion = confus ,Sensitivity= Se,
              Specificity=sp, precision = Pr,recall = Re, JacardIndex = Ja))
}

```

```{r}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
```

```{r}
#Naive bayes # The important class been detcted less
confusion_matrix_fun(test$y,preds_naive_bayes)
```

## Q3:

Write your own function that draws a response operating curve (ROC). Draw ROCs for both classifiers

```{r}
Roc_plot <- function(model,title){
  t <- thresholder(model, threshold = seq(0,1,by = 0.05))
  g <- ggplot(t, aes(y = Sensitivity,x = 1-Specificity)) + geom_line() + labs(title = title, subtitle = "Manual computing by intervals of 0.05")
  return(g)
}
Roc_plot(gbm_model_hyperparmeters,"ROC for Gardient Boosting Model")
```

```{r}

Roc_plot(naive_bayes_model,"ROC for naive bayes Model")

```

## Q4:

For one of your classifiers, display four examples that were classified incorrectly. Can you see what
made these examples hard for the classifier?

```{r}
incorrectly_naive_bayes <- sample_n(test[-which(test$y == preds_naive_bayes),],4)
incorrectly_naive_bayes
incorrectly_naive_bayes$y
```

Naive Bayes classifier assumes the features are independent, the probabilities are incorrect if this assumption is not correct.

We work on MNIST data, cause of it we know what our data include. We can be sure that not all of the feature are independent, the feature which define the outline of the numbers 3 and 8 are dependent with each other what can be misleading.


```{r}
naive_mean <- apply(train4000[,-718],1,mean)
naive_sd <- apply(train4000[,-718],1,sd)
naive_incorecct_ouylier <- incorrectly_naive_bayes[,-718]
naive_incorecct_ouylier <- sweep(naive_incorecct_ouylier,2,naive_mean)
naive_incorecct_ouylier <- sweep(naive_incorecct_ouylier,2,naive_sd, '/')
```

```{r}
f1 <- which(c(abs(naive_incorecct_ouylier[1,]) > 1.95) == TRUE )
f2 <- which(c(abs(naive_incorecct_ouylier[2,]) > 1.95) == TRUE )
f3 <- which(c(abs(naive_incorecct_ouylier[3,]) > 1.95) == TRUE )
f4 <- which(c(abs(naive_incorecct_ouylier[4,]) > 1.95) == TRUE )

```

## Q5:

Here is an image of a white digit (the digit 3) on a dark background. Do you expect both of your fitted
classifiers to work well on this image? Why or why not?
[Hint: Think how would this image be coded into numbers? what would happen if you try to classify
using your method?]


For simplifying how the the number will be coded: Lets take the mean of each feature,with y = 3.
our new picture will be coded for every feature in the area of (255 - E(feature|Y = 3)).
There can be errors, but that only for the example.

The first classifier: Naive bayes might predict right ?
The second classifier: Gradient boosting will probably predict wrong, we need to remember that that classifier learning on trees, and in this case we will almost always take the wrong turn in the cross section.

```{r}
y3 <- train4000 %>% filter(y == 'X3')
y3 <- apply(y3[,-718],2,mean)
y3 <- 255 - y3
y3 <- t(as.data.frame(y3))
```

```{r}
predict(naive_bayes_model,y3)
predict(gbm_model_hyperparmeters,y3)
```




