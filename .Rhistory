Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive Prediction (8)","Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
return(list(confusion = confus , precision = Pr,recall = Re, JacardIndex = Ja))
}
confusion_matrix_fun(test$y,pred_gbm_model)
confusion_matrix_fun(test$y,preds_naive_bayes)
#Gardiante boosting
confusion_matrix_fun(test$y,pred_gbm_model)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 8)]
Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3),
sum(Negative_line == 8)/length(Negative_line),
sum(Negative_line == 3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3),
sum(Postive_line == 8)/length(Postive_line),
sum(Postive_line == 3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive Prediction (8)","Postive Prediction (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
return(list(confusion = confus , precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 8)]
Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3),
sum(Negative_line == 8)/length(Negative_line),
sum(Negative_line == 3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3),
sum(Postive_line == 8)/length(Postive_line),
sum(Postive_line == 3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtiven (8)"," % Postive (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
return(list(confusion = confus , precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 8)]
Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3),
sum(Negative_line == 8)/length(Negative_line),
sum(Negative_line == 3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3),
sum(Postive_line == 8)/length(Postive_line),
sum(Postive_line == 3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive /n Prediction (8)"," % Postive /n Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
return(list(confusion = confus , precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 8)]
Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3),
sum(Negative_line == 8)/length(Negative_line),
sum(Negative_line == 3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3),
sum(Postive_line == 8)/length(Postive_line),
sum(Postive_line == 3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
return(list(confusion = confus , precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
#Naive bayes # The important class been detcted less
confusion_matrix_fun(test$y,preds_naive_bayes)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 8)]
Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3),
sum(Negative_line == 8)/length(Negative_line),
sum(Negative_line == 3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3),
sum(Postive_line == 8)/length(Postive_line),
sum(Postive_line == 3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
Se <- confus[2,2] / (confus[2,2] + confus[2,1]) # TP / TP+FN
sp <- confus[1,1] / (confus[1,1] + confus[1,2]) # TN/ TN+FP
return(list(confusion = confus ,Sensitivity= Se,
Specificity=sp, precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
#Naive bayes # The important class been detcted less
confusion_matrix_fun(test$y,preds_naive_bayes)
gbm_model_hyperparmeters$results
View(gbm_model_hyperparmeters)
gbm_model_hyperparmeters$modelInfo
gbm_model_hyperparmeters$pred
gbm_model_hyperparmeters$pred
?thresholder
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.05))
return(t)
}
Roc_plot(gbm_model_hyperparmeters)
options(scipen = 999)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(readr)
library(readxl)
library(glmnet)
library(grid)
library(gridExtra)
library(cowplot)
library(xgboost)
library(caret)
library(rsample)
# load image files
load_image_file = function(filename) {
ret = list()
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
close(f)
data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
}
# load label files
load_label_file = function(filename) {
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
close(f)
y
}
# load images
train = load_image_file("train-images-idx3-ubyte")
test  = load_image_file("t10k-images-idx3-ubyte")
# load labels
train$y = as.factor(load_label_file("train-labels-idx1-ubyte"))
test$y  = as.factor(load_label_file("t10k-labels-idx1-ubyte"))
# view test image
#show_digit(train[10000, ])
# use only 4000 for training. train on the same proportion of y
zero_var <- round(apply(train[,-785],2,var),5)
zero_var <- zero_var != 0
p <- c(which(zero_var == T),785)
test <- test[,p] %>% filter(y %in% list(3,8))
test[1:784,] <- test[1:784,]
train <- train[,p] %>% filter(y %in% list(3,8))
train[1:784,] <- train[1:784,]
train$y <- as.factor(train$y)
set.seed(1342)
train_sampling <- initial_split(train,prop=4000/length(train[,1]), strata="y")
train_sampling$id
train4000 <- training(train_sampling)
train4000$y <- factor(train4000$y)
test$y <- factor(test$y)
#train4000 %>% group_by(y) %>% count()
# estimating G from the data #presntaion 11a # check kernel, and band # non-parmetric
naive_bayes_model <- train(y ~., data = train4000 ,method = 'naive_bayes',
trControl = trainControl(classProbs = TRUE))
?make.names
train4000$y <- factor(make.names(train4000$y))
test$y <- factor(test$y)
test$y <- factor(make.names(test$y))
# estimating G from the data #presntaion 11a # check kernel, and band # non-parmetric
naive_bayes_model <- train(y ~., data = train4000 ,method = 'naive_bayes',
trControl = trainControl(classProbs = TRUE))
preds_naive_bayes <- predict(naive_bayes_model, newdata = test)
gbm_model_hyperparmeters <- train(y ~ ., data = train4000, method = 'xgbTree',
trControl = trainControl("cv", number = 3, classProbs = TRUE))
gbm_model_hyperparmeters$bestTune
pred_gbm_model <- predict(gbm_model_hyperparmeters,test)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 8)]
Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3),
sum(Negative_line == 8)/length(Negative_line),
sum(Negative_line == 3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3),
sum(Postive_line == 8)/length(Postive_line),
sum(Postive_line == 3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
Se <- confus[2,2] / (confus[2,2] + confus[2,1]) # TP / TP+FN
sp <- confus[1,1] / (confus[1,1] + confus[1,2]) # TN/ TN+FP
return(list(confusion = confus ,Sensitivity= Se,
Specificity=sp, precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
#Naive bayes # The important class been detcted less
confusion_matrix_fun(test$y,preds_naive_bayes)
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.05))
g <- ggplot(t, aes(Sensitivity,1-Specificity))
return(g)
}
Roc_plot(gbm_model_hyperparmeters)
?trainControl
# load image files
load_image_file = function(filename) {
ret = list()
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
close(f)
data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
}
# load label files
load_label_file = function(filename) {
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
close(f)
y
}
# load images
train = load_image_file("train-images-idx3-ubyte")
test  = load_image_file("t10k-images-idx3-ubyte")
# load labels
train$y = as.factor(load_label_file("train-labels-idx1-ubyte"))
test$y  = as.factor(load_label_file("t10k-labels-idx1-ubyte"))
# view test image
#show_digit(train[10000, ])
# use only 4000 for training. train on the same proportion of y
zero_var <- round(apply(train[,-785],2,var),5)
zero_var <- zero_var != 0
p <- c(which(zero_var == T),785)
test <- test[,p] %>% filter(y %in% list(3,8))
test[1:784,] <- test[1:784,]
train <- train[,p] %>% filter(y %in% list(3,8))
train[1:784,] <- train[1:784,]
train$y <- as.factor(train$y)
set.seed(1342)
train_sampling <- initial_split(train,prop=4000/length(train[,1]), strata="y")
train_sampling$id
train4000 <- training(train_sampling)
train4000$y <- factor(make.names(train4000$y))
test$y <- factor(make.names(test$y))
#train4000 %>% group_by(y) %>% count()
# estimating G from the data #presntaion 11a # check kernel, and band # non-parmetric
naive_bayes_model <- train(y ~., data = train4000 ,method = 'naive_bayes',
trControl = trainControl(classProbs = TRUE, savePredictions =TRUE))
preds_naive_bayes <- predict(naive_bayes_model, newdata = test)
gbm_model_hyperparmeters <- train(y ~ ., data = train4000, method = 'xgbTree',
trControl = trainControl("cv", number = 3, classProbs = TRUE,
savePredictions = TRUE))
gbm_model_hyperparmeters$bestTune
pred_gbm_model <- predict(gbm_model_hyperparmeters,test)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 8)]
Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3),
sum(Negative_line == 8)/length(Negative_line),
sum(Negative_line == 3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3),
sum(Postive_line == 8)/length(Postive_line),
sum(Postive_line == 3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
Se <- confus[2,2] / (confus[2,2] + confus[2,1]) # TP / TP+FN
sp <- confus[1,1] / (confus[1,1] + confus[1,2]) # TN/ TN+FP
return(list(confusion = confus ,Sensitivity= Se,
Specificity=sp, precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
#Naive bayes # The important class been detcted less
confusion_matrix_fun(test$y,preds_naive_bayes)
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.05))
g <- ggplot(t, aes(Sensitivity,1-Specificity))
return(g)
}
Roc_plot(gbm_model_hyperparmeters)
# estimating G from the data #presntaion 11a # check kernel, and band # non-parmetric
naive_bayes_model <- train(y ~., data = train4000 ,method = 'naive_bayes',
trControl = trainControl(classProbs = TRUE))
# estimating G from the data #presntaion 11a # check kernel, and band # non-parmetric
naive_bayes_model <- train(y ~., data = train4000 ,method = 'naive_bayes',
trControl = trainControl(classProbs = TRUE))
preds_naive_bayes <- predict(naive_bayes_model, newdata = test)
gbm_model_hyperparmeters <- train(y ~ ., data = train4000, method = 'xgbTree',
trControl = trainControl("cv", number = 3, classProbs = TRUE))
gbm_model_hyperparmeters$bestTune
pred_gbm_model <- predict(gbm_model_hyperparmeters,test)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 8)]
Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3),
sum(Negative_line == 8)/length(Negative_line),
sum(Negative_line == 3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3),
sum(Postive_line == 8)/length(Postive_line),
sum(Postive_line == 3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
Se <- confus[2,2] / (confus[2,2] + confus[2,1]) # TP / TP+FN
sp <- confus[1,1] / (confus[1,1] + confus[1,2]) # TN/ TN+FP
return(list(confusion = confus ,Sensitivity= Se,
Specificity=sp, precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
#Naive bayes # The important class been detcted less
confusion_matrix_fun(test$y,preds_naive_bayes)
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.05))
g <- ggplot(t, aes(Sensitivity,1-Specificity))
return(g)
}
Roc_plot(gbm_model_hyperparmeters)
# estimating G from the data #presntaion 11a # check kernel, and band # non-parmetric
naive_bayes_model <- train(y ~., data = train4000 ,method = 'naive_bayes',
trControl = trainControl(classProbs = TRUE,,savePredictions= "final"))
preds_naive_bayes <- predict(naive_bayes_model, newdata = test)
gbm_model_hyperparmeters <- train(y ~ ., data = train4000, method = 'xgbTree',
trControl = trainControl("cv", number = 3,
classProbs = TRUE,savePredictions= "final"))
# estimating G from the data #presntaion 11a # check kernel, and band # non-parmetric
naive_bayes_model <- train(y ~., data = train4000 ,method = 'naive_bayes',
trControl = trainControl(classProbs = TRUE,savePredictions= "final"))
preds_naive_bayes <- predict(naive_bayes_model, newdata = test)
gbm_model_hyperparmeters <- train(y ~ ., data = train4000, method = 'xgbTree',
trControl = trainControl("cv", number = 3,
classProbs = TRUE,savePredictions= "final"))
gbm_model_hyperparmeters$bestTune
pred_gbm_model <- predict(gbm_model_hyperparmeters,test)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 8)]
Postive_line <- Yhat[which(Y == 3)]
Negative_line <- c(sum(Negative_line == 8),
sum(Negative_line == 3),
sum(Negative_line == 8)/length(Negative_line),
sum(Negative_line == 3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == 8),
sum(Postive_line == 3),
sum(Postive_line == 8)/length(Postive_line),
sum(Postive_line == 3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
Se <- confus[2,2] / (confus[2,2] + confus[2,1]) # TP / TP+FN
sp <- confus[1,1] / (confus[1,1] + confus[1,2]) # TN/ TN+FP
return(list(confusion = confus ,Sensitivity= Se,
Specificity=sp, precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
#Naive bayes # The important class been detcted less
confusion_matrix_fun(test$y,preds_naive_bayes)
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.05))
g <- ggplot(t, aes(Sensitivity,1-Specificity))
return(g)
}
Roc_plot(gbm_model_hyperparmeters)
pred_gbm_model
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == X8)]
Postive_line <- Yhat[which(Y == X3)]
Negative_line <- c(sum(Negative_line == X8),
sum(Negative_line == X3),
sum(Negative_line == X8)/length(Negative_line),
sum(Negative_line == X3)/length(Negative_line))
Postive_line <- c(sum(Postive_line == X8),
sum(Postive_line == X3),
sum(Postive_line == X8)/length(Postive_line),
sum(Postive_line == X3)/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
Se <- confus[2,2] / (confus[2,2] + confus[2,1]) # TP / TP+FN
sp <- confus[1,1] / (confus[1,1] + confus[1,2]) # TN/ TN+FP
return(list(confusion = confus ,Sensitivity= Se,
Specificity=sp, precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
confusion_matrix_fun <- function(Y,Yhat){
Negative_line <- Yhat[which(Y == 'X8')]
Postive_line <- Yhat[which(Y == 'X3')]
Negative_line <- c(sum(Negative_line == 'X8'),
sum(Negative_line == 'X3'),
sum(Negative_line == 'X8')/length(Negative_line),
sum(Negative_line == 'X3')/length(Negative_line))
Postive_line <- c(sum(Postive_line == 'X8'),
sum(Postive_line == 'X3'),
sum(Postive_line == 'X8')/length(Postive_line),
sum(Postive_line == 'X3')/length(Postive_line))
confus <- rbind(Negative_line,Postive_line)
colnames(confus) <- c("Negtive (8)","Postive (3)","% Negtive Prediction (8)"," % Postive Prediction (3)")
row.names(confus) <- c("Negtive (8)","Postive (3)")
Pr <- confus[2,2] / sum(confus[,2])
Re <- confus[2,2] / sum(confus[2,])
Ja <- confus[2,2] / (confus[1,2] + confus[1,1] + confus[2,1])
Se <- confus[2,2] / (confus[2,2] + confus[2,1]) # TP / TP+FN
sp <- confus[1,1] / (confus[1,1] + confus[1,2]) # TN/ TN+FP
return(list(confusion = confus ,Sensitivity= Se,
Specificity=sp, precision = Pr,recall = Re, JacardIndex = Ja))
}
#Gardiante boosting No overfiting
confusion_matrix_fun(test$y,pred_gbm_model)
#Naive bayes # The important class been detcted less
confusion_matrix_fun(test$y,preds_naive_bayes)
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.05))
g <- ggplot(t, aes(Sensitivity,1-Specificity))
return(g)
}
Roc_plot(gbm_model_hyperparmeters)
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.05))
g <- ggplot(t, aes(Sensitivity,1-Specificity))
return(t)
}
Roc_plot(gbm_model_hyperparmeters)
?seq
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.005))
g <- ggplot(t, aes(Sensitivity,1-Specificity))
return(t)
}
Roc_plot(gbm_model_hyperparmeters)
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.005))
g <- ggplot(t, aes(Sensitivity,1-Specificity)) + geom_line()
return(g)
}
Roc_plot(gbm_model_hyperparmeters)
Roc_plot <- function(model){
t <- thresholder(model, threshold = seq(0,1,by = 0.05))
g <- ggplot(t, aes(y = Sensitivity,x = 1-Specificity)) + geom_line() + labs(title = "ROC", subtitle = "Manual computing by intervals of 0.05")
return(g)
}
Roc_plot(gbm_model_hyperparmeters)
Roc_plot <- function(model,title){
t <- thresholder(model, threshold = seq(0,1,by = 0.05))
g <- ggplot(t, aes(y = Sensitivity,x = 1-Specificity)) + geom_line() + labs(title = "ROC", subtitle = "Manual computing by intervals of 0.05")
return(g)
}
Roc_plot(gbm_model_hyperparmeters,"ROC for Gardient Boosting Model")
Roc_plot(naive_bayes_model,"ROC for naive bayes Model")
